{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fcad0ca",
   "metadata": {},
   "source": [
    "# TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "75f98cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>hurricane_maria</td>\n",
       "      <td>912341945430798336</td>\n",
       "      <td>912341945430798336_0</td>\n",
       "      <td>11am #Maria update: tropical storm force winds...</td>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912341945...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909936612196454403</td>\n",
       "      <td>909936612196454403_0</td>\n",
       "      <td>RT @WhereTraveler: Hurricane Irma Survival Gui...</td>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099366121...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>hurricane_harvey</td>\n",
       "      <td>905581858485882881</td>\n",
       "      <td>905581858485882881_0</td>\n",
       "      <td>Thinking of doing a #DNA test? Buy a @FamilyTr...</td>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905581858...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909879746598723590</td>\n",
       "      <td>909879746598723590_0</td>\n",
       "      <td>Now blaming Hurricane Irma on Ursula the Witch...</td>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098797465...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>mexico_earthquake</td>\n",
       "      <td>910700764808564736</td>\n",
       "      <td>910700764808564736_0</td>\n",
       "      <td>Earthquake in Mexico rocks PH Embassy, no Fili...</td>\n",
       "      <td>data_image/mexico_earthquake/21_9_2017/9107007...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 event_name            tweet_id              image_id  \\\n",
       "0      california_wildfires  917791291823591425  917791291823591425_0   \n",
       "1      california_wildfires  917791291823591425  917791291823591425_1   \n",
       "2      california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3      california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4      california_wildfires  917793137925459968  917793137925459968_2   \n",
       "...                     ...                 ...                   ...   \n",
       "13603       hurricane_maria  912341945430798336  912341945430798336_0   \n",
       "13604        hurricane_irma  909936612196454403  909936612196454403_0   \n",
       "13605      hurricane_harvey  905581858485882881  905581858485882881_0   \n",
       "13606        hurricane_irma  909879746598723590  909879746598723590_0   \n",
       "13607     mexico_earthquake  910700764808564736  910700764808564736_0   \n",
       "\n",
       "                                              tweet_text  \\\n",
       "0      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "1      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "2      RT @KAKEnews: California wildfires destroy mor...   \n",
       "3      RT @KAKEnews: California wildfires destroy mor...   \n",
       "4      RT @KAKEnews: California wildfires destroy mor...   \n",
       "...                                                  ...   \n",
       "13603  11am #Maria update: tropical storm force winds...   \n",
       "13604  RT @WhereTraveler: Hurricane Irma Survival Gui...   \n",
       "13605  Thinking of doing a #DNA test? Buy a @FamilyTr...   \n",
       "13606  Now blaming Hurricane Irma on Ursula the Witch...   \n",
       "13607  Earthquake in Mexico rocks PH Embassy, no Fili...   \n",
       "\n",
       "                                                   image            label  \\\n",
       "0      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "1      data_image/california_wildfires/10_10_2017/917...  not_informative   \n",
       "2      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "3      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "4      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "...                                                  ...              ...   \n",
       "13603  data_image/hurricane_maria/25_9_2017/912341945...      informative   \n",
       "13604  data_image/hurricane_irma/19_9_2017/9099366121...      informative   \n",
       "13605  data_image/hurricane_harvey/7_9_2017/905581858...  not_informative   \n",
       "13606  data_image/hurricane_irma/18_9_2017/9098797465...      informative   \n",
       "13607  data_image/mexico_earthquake/21_9_2017/9107007...      informative   \n",
       "\n",
       "        label_text      label_image label_text_image  \n",
       "0      informative      informative         Positive  \n",
       "1      informative  not_informative         Negative  \n",
       "2      informative      informative         Positive  \n",
       "3      informative      informative         Positive  \n",
       "4      informative      informative         Positive  \n",
       "...            ...              ...              ...  \n",
       "13603  informative      informative         Positive  \n",
       "13604  informative  not_informative         Negative  \n",
       "13605  informative  not_informative         Negative  \n",
       "13606  informative  not_informative         Negative  \n",
       "13607  informative      informative         Positive  \n",
       "\n",
       "[13608 rows x 9 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Downloads\\\\CRISIS_1\\\\task_informative_text_img_train.csv\", sep=',')\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aec7a88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=df1[['tweet_text','label_text']]\n",
    "df3=selected_columns.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a7b6043a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>11am #Maria update: tropical storm force winds...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>RT @WhereTraveler: Hurricane Irma Survival Gui...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>Thinking of doing a #DNA test? Buy a @FamilyTr...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>Now blaming Hurricane Irma on Ursula the Witch...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>Earthquake in Mexico rocks PH Embassy, no Fili...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   label_text\n",
       "0      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  informative\n",
       "1      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  informative\n",
       "2      RT @KAKEnews: California wildfires destroy mor...  informative\n",
       "3      RT @KAKEnews: California wildfires destroy mor...  informative\n",
       "4      RT @KAKEnews: California wildfires destroy mor...  informative\n",
       "...                                                  ...          ...\n",
       "13603  11am #Maria update: tropical storm force winds...  informative\n",
       "13604  RT @WhereTraveler: Hurricane Irma Survival Gui...  informative\n",
       "13605  Thinking of doing a #DNA test? Buy a @FamilyTr...  informative\n",
       "13606  Now blaming Hurricane Irma on Ursula the Witch...  informative\n",
       "13607  Earthquake in Mexico rocks PH Embassy, no Fili...  informative\n",
       "\n",
       "[13608 rows x 2 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a32c7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = []\n",
    "\n",
    "for i in range(df3.shape[0]):\n",
    "    if (df3.iloc[i]['label_text']=='informative'):\n",
    "        code.append(1)\n",
    "    elif (df3.iloc[i]['label_text']=='not_informative'):\n",
    "        code.append(0)\n",
    "    else:\n",
    "#         poke_type.append('NaN')\n",
    "        code.append('NaN')\n",
    "df3['code'] = code\n",
    "new_df = df3\n",
    "\n",
    "new_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "95693e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>11am #Maria update: tropical storm force winds...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>RT @WhereTraveler: Hurricane Irma Survival Gui...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>Thinking of doing a #DNA test? Buy a @FamilyTr...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>Now blaming Hurricane Irma on Ursula the Witch...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>Earthquake in Mexico rocks PH Embassy, no Fili...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text   label_text  code\n",
       "0      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  informative     1\n",
       "1      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...  informative     1\n",
       "2      RT @KAKEnews: California wildfires destroy mor...  informative     1\n",
       "3      RT @KAKEnews: California wildfires destroy mor...  informative     1\n",
       "4      RT @KAKEnews: California wildfires destroy mor...  informative     1\n",
       "...                                                  ...          ...   ...\n",
       "13603  11am #Maria update: tropical storm force winds...  informative     1\n",
       "13604  RT @WhereTraveler: Hurricane Irma Survival Gui...  informative     1\n",
       "13605  Thinking of doing a #DNA test? Buy a @FamilyTr...  informative     1\n",
       "13606  Now blaming Hurricane Irma on Ursula the Witch...  informative     1\n",
       "13607  Earthquake in Mexico rocks PH Embassy, no Fili...  informative     1\n",
       "\n",
       "[13608 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f84ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df= train_test_split(new_df, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bc34b05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-138-6946dafe07e6>:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['tweet_text'] = train_df['tweet_text'].apply(lambda x: clean_text(x) )\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import SnowballStemmer\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stopwords.words(\"english\"))\n",
    "stop_wrds=np.array(stop_words)\n",
    "# stemmer=SnowballStemmer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "def clean_text(each_text):\n",
    "\n",
    "    # remove URL from text\n",
    "    each_text_no_url = re.sub(r\"http\\S+\", \"\", each_text)\n",
    "    \n",
    "    # remove numbers from text\n",
    "    text_no_num = re.sub(r'\\d+', '', each_text_no_url)\n",
    "\n",
    "    # tokenize each text\n",
    "    word_tokens = word_tokenize(text_no_num)\n",
    "    \n",
    "    # remove sptial character\n",
    "    clean_text = []\n",
    "    for word in word_tokens:\n",
    "        clean_text.append(\"\".join([e for e in word if e.isalnum()]))\n",
    "\n",
    "    # remove stop words and lower\n",
    "    text_with_no_stop_word = [w.lower() for w in clean_text if not w in stop_words]  \n",
    "\n",
    "    # do stemming\n",
    "    stemmed_text = [stemmer.stem(w) for w in text_with_no_stop_word]\n",
    "    \n",
    "    return \" \".join(\" \".join(stemmed_text).split())\n",
    "\n",
    "\n",
    "train_df['tweet_text'] = train_df['tweet_text'].apply(lambda x: clean_text(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e4b86371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>immigr communiti fear harvey relief effort cou...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>russia look expand influenc latin america aid ...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>rt thebrasil adam amp georg potenci meme lolla...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>maria stay offshor close enough brush deliv ou...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>hurrican maria power outag put old vulner ri us</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>antonio medina new train buddi new york irma w...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>so thank god protect us irma if tree fallen to...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>the fuck do u mean watch i m over here hope a ...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>irma still power cat storm sustain wind mph it...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>usatoday raw over known dead iran earthquak</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text       label_text  \\\n",
       "11191  immigr communiti fear harvey relief effort cou...  not_informative   \n",
       "7707   russia look expand influenc latin america aid ...      informative   \n",
       "182    rt thebrasil adam amp georg potenci meme lolla...  not_informative   \n",
       "1724   maria stay offshor close enough brush deliv ou...      informative   \n",
       "7911     hurrican maria power outag put old vulner ri us      informative   \n",
       "...                                                  ...              ...   \n",
       "2934   antonio medina new train buddi new york irma w...      informative   \n",
       "10383  so thank god protect us irma if tree fallen to...      informative   \n",
       "6618   the fuck do u mean watch i m over here hope a ...  not_informative   \n",
       "8510   irma still power cat storm sustain wind mph it...      informative   \n",
       "13444        usatoday raw over known dead iran earthquak      informative   \n",
       "\n",
       "       code  \n",
       "11191     0  \n",
       "7707      1  \n",
       "182       0  \n",
       "1724      1  \n",
       "7911      1  \n",
       "...     ...  \n",
       "2934      1  \n",
       "10383     1  \n",
       "6618      0  \n",
       "8510      1  \n",
       "13444     1  \n",
       "\n",
       "[10886 rows x 3 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "62cf48f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>label_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>Don't forget about Puerto Rico. https://t.co/K...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>Weekend Update on Hurricane Maria - SNL WATCH ...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Take me back to the good old days in my Puerto...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Mas adam don't forget mimpY mora, med bubuG á½...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>photos day after Irma... https://t.co/cCC5PHyl3r</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>In Newark where local leaders are announcing r...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Ocean Park, a moment ago, still under water. P...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>With a forecast track hauntingly close to #Irm...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>#art - Tree of Life Bildinsperation Antonio Mo...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7692</th>\n",
       "      <td>Hurricane Irma: Symphony of Compensation https...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2722 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text       label_text  \\\n",
       "6334   Don't forget about Puerto Rico. https://t.co/K...  not_informative   \n",
       "10738  Weekend Update on Hurricane Maria - SNL WATCH ...      informative   \n",
       "3986   Take me back to the good old days in my Puerto...  not_informative   \n",
       "438    Mas adam don't forget mimpY mora, med bubuG á½...  not_informative   \n",
       "2803    photos day after Irma... https://t.co/cCC5PHyl3r      informative   \n",
       "...                                                  ...              ...   \n",
       "1775   In Newark where local leaders are announcing r...      informative   \n",
       "1645   Ocean Park, a moment ago, still under water. P...      informative   \n",
       "6420   With a forecast track hauntingly close to #Irm...      informative   \n",
       "13236  #art - Tree of Life Bildinsperation Antonio Mo...  not_informative   \n",
       "7692   Hurricane Irma: Symphony of Compensation https...      informative   \n",
       "\n",
       "       code  \n",
       "6334      0  \n",
       "10738     1  \n",
       "3986      0  \n",
       "438       0  \n",
       "2803      1  \n",
       "...     ...  \n",
       "1775      1  \n",
       "1645      1  \n",
       "6420      1  \n",
       "13236     0  \n",
       "7692      1  \n",
       "\n",
       "[2722 rows x 3 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a1457c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "train_text = train_df['tweet_text']\n",
    "y = train_df['code']\n",
    "train_df.drop('label_text',axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6410ee53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>immigr communiti fear harvey relief effort cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>russia look expand influenc latin america aid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>rt thebrasil adam amp georg potenci meme lolla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>maria stay offshor close enough brush deliv ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>hurrican maria power outag put old vulner ri us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>antonio medina new train buddi new york irma w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>so thank god protect us irma if tree fallen to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>the fuck do u mean watch i m over here hope a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>irma still power cat storm sustain wind mph it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>usatoday raw over known dead iran earthquak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  code\n",
       "11191  immigr communiti fear harvey relief effort cou...     0\n",
       "7707   russia look expand influenc latin america aid ...     1\n",
       "182    rt thebrasil adam amp georg potenci meme lolla...     0\n",
       "1724   maria stay offshor close enough brush deliv ou...     1\n",
       "7911     hurrican maria power outag put old vulner ri us     1\n",
       "...                                                  ...   ...\n",
       "2934   antonio medina new train buddi new york irma w...     1\n",
       "10383  so thank god protect us irma if tree fallen to...     1\n",
       "6618   the fuck do u mean watch i m over here hope a ...     0\n",
       "8510   irma still power cat storm sustain wind mph it...     1\n",
       "13444        usatoday raw over known dead iran earthquak     1\n",
       "\n",
       "[10886 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5ed9546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 10\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "64d6c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c06ff738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14387"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(train_text)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1f558790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10886, 10)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, GRU, SimpleRNN, Embedding, Flatten\n",
    "sequences = tokenizer.texts_to_sequences(train_text)\n",
    "X = pad_sequences(sequences, maxlen=max_len)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f1b33284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "# test_df.drop('code',axis = 1,inplace=True)\n",
    "test_df.drop('label_text',axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d11f46fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>immigr communiti fear harvey relief effort cou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>russia look expand influenc latin america aid ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>rt thebrasil adam amp georg potenci meme lolla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>maria stay offshor close enough brush deliv ou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>hurrican maria power outag put old vulner ri us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>antonio medina new train buddi new york irma w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>so thank god protect us irma if tree fallen to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>the fuck do u mean watch i m over here hope a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>irma still power cat storm sustain wind mph it...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>usatoday raw over known dead iran earthquak</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  code\n",
       "11191  immigr communiti fear harvey relief effort cou...     0\n",
       "7707   russia look expand influenc latin america aid ...     1\n",
       "182    rt thebrasil adam amp georg potenci meme lolla...     0\n",
       "1724   maria stay offshor close enough brush deliv ou...     1\n",
       "7911     hurrican maria power outag put old vulner ri us     1\n",
       "...                                                  ...   ...\n",
       "2934   antonio medina new train buddi new york irma w...     1\n",
       "10383  so thank god protect us irma if tree fallen to...     1\n",
       "6618   the fuck do u mean watch i m over here hope a ...     0\n",
       "8510   irma still power cat storm sustain wind mph it...     1\n",
       "13444        usatoday raw over known dead iran earthquak     1\n",
       "\n",
       "[10886 rows x 2 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b3b94f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>Don't forget about Puerto Rico. https://t.co/K...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>Weekend Update on Hurricane Maria - SNL WATCH ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>Take me back to the good old days in my Puerto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Mas adam don't forget mimpY mora, med bubuG á½...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>photos day after Irma... https://t.co/cCC5PHyl3r</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>In Newark where local leaders are announcing r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>Ocean Park, a moment ago, still under water. P...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>With a forecast track hauntingly close to #Irm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>#art - Tree of Life Bildinsperation Antonio Mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7692</th>\n",
       "      <td>Hurricane Irma: Symphony of Compensation https...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text  code\n",
       "6334   Don't forget about Puerto Rico. https://t.co/K...     0\n",
       "10738  Weekend Update on Hurricane Maria - SNL WATCH ...     1\n",
       "3986   Take me back to the good old days in my Puerto...     0\n",
       "438    Mas adam don't forget mimpY mora, med bubuG á½...     0\n",
       "2803    photos day after Irma... https://t.co/cCC5PHyl3r     1\n",
       "...                                                  ...   ...\n",
       "1775   In Newark where local leaders are announcing r...     1\n",
       "1645   Ocean Park, a moment ago, still under water. P...     1\n",
       "6420   With a forecast track hauntingly close to #Irm...     1\n",
       "13236  #art - Tree of Life Bildinsperation Antonio Mo...     0\n",
       "7692   Hurricane Irma: Symphony of Compensation https...     1\n",
       "\n",
       "[2722 rows x 2 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "95136556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2722, 10)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = tokenizer.texts_to_sequences(test_df['tweet_text'])\n",
    "test_data = pad_sequences(test_data, maxlen=max_len)\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b68bfdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10886, 1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(y).reshape((-1,1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6b64b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "31a952c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9253, 10)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3ae87e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size=32, epochs=50):\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    history=model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "    print('-' * 100)\n",
    "    print('Test data')\n",
    "    model.evaluate(X_test, y_test)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2a64afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_validation_and_accuracy(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs_plot = np.arange(1, len(loss) + 1)\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(epochs_plot, acc, 'r', label='Training acc')\n",
    "    plt.plot(epochs_plot, val_acc, 'b', label='Validation acc')\n",
    "    plt.plot(epochs_plot, loss, 'r:', label='Training loss')\n",
    "    plt.plot(epochs_plot, val_loss, 'b:', label='Validation loss')\n",
    "    plt.title('Validation and accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "889873e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 10, 32)            320000    \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 320)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 16)                5136      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 325,281\n",
      "Trainable params: 325,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 32, input_length=max_len),\n",
    "    Flatten(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a6701831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.6119 - acc: 0.7059 - val_loss: 0.5480 - val_acc: 0.6964\n",
      "Epoch 2/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.5218 - acc: 0.7098 - val_loss: 0.4832 - val_acc: 0.6964\n",
      "Epoch 3/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4549 - acc: 0.7302 - val_loss: 0.4655 - val_acc: 0.7974\n",
      "Epoch 4/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.4026 - acc: 0.8530 - val_loss: 0.4577 - val_acc: 0.8125\n",
      "Epoch 5/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.3499 - acc: 0.8877 - val_loss: 0.4658 - val_acc: 0.8125\n",
      "Epoch 6/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2965 - acc: 0.9146 - val_loss: 0.4984 - val_acc: 0.8055\n",
      "Epoch 7/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2427 - acc: 0.9333 - val_loss: 0.5333 - val_acc: 0.7909\n",
      "Epoch 8/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.2044 - acc: 0.9491 - val_loss: 0.5957 - val_acc: 0.7790\n",
      "Epoch 9/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1808 - acc: 0.9557 - val_loss: 0.6570 - val_acc: 0.7720\n",
      "Epoch 10/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1533 - acc: 0.9650 - val_loss: 0.7442 - val_acc: 0.7720A: 0s - loss: 0.1518 - acc: 0.9\n",
      "Epoch 11/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1339 - acc: 0.9684 - val_loss: 0.8868 - val_acc: 0.7715TA: 0s - loss: 0.1364 - acc: 0\n",
      "Epoch 12/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1183 - acc: 0.9718 - val_loss: 0.9970 - val_acc: 0.7661\n",
      "Epoch 13/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.1044 - acc: 0.9754 - val_loss: 1.1155 - val_acc: 0.7612\n",
      "Epoch 14/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0976 - acc: 0.9754 - val_loss: 1.2577 - val_acc: 0.7693\n",
      "Epoch 15/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0900 - acc: 0.9774 - val_loss: 1.3028 - val_acc: 0.7542\n",
      "Epoch 16/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0780 - acc: 0.9808 - val_loss: 1.4237 - val_acc: 0.7450\n",
      "Epoch 17/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0775 - acc: 0.9789 - val_loss: 1.6339 - val_acc: 0.7526\n",
      "Epoch 18/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0746 - acc: 0.9791 - val_loss: 1.8414 - val_acc: 0.7553\n",
      "Epoch 19/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0659 - acc: 0.9819 - val_loss: 2.0336 - val_acc: 0.7634\n",
      "Epoch 20/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0649 - acc: 0.9814 - val_loss: 2.1235 - val_acc: 0.7542\n",
      "Epoch 21/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0668 - acc: 0.9803 - val_loss: 2.3289 - val_acc: 0.7558\n",
      "Epoch 22/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0600 - acc: 0.9823 - val_loss: 2.5273 - val_acc: 0.7580\n",
      "Epoch 23/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0594 - acc: 0.9827 - val_loss: 2.9782 - val_acc: 0.7704\n",
      "Epoch 24/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0619 - acc: 0.9808 - val_loss: 3.1053 - val_acc: 0.7661\n",
      "Epoch 25/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0531 - acc: 0.9841 - val_loss: 3.2640 - val_acc: 0.7639\n",
      "Epoch 26/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0559 - acc: 0.9826 - val_loss: 3.6128 - val_acc: 0.7709\n",
      "Epoch 27/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0497 - acc: 0.9846 - val_loss: 3.7493 - val_acc: 0.7628\n",
      "Epoch 28/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0528 - acc: 0.9824 - val_loss: 4.1593 - val_acc: 0.7677\n",
      "Epoch 29/50\n",
      "232/232 [==============================] - 1s 4ms/step - loss: 0.0487 - acc: 0.9843 - val_loss: 4.4172 - val_acc: 0.7639\n",
      "Epoch 30/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0416 - acc: 0.9869 - val_loss: 4.9279 - val_acc: 0.7639\n",
      "Epoch 31/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0422 - acc: 0.9864 - val_loss: 5.5207 - val_acc: 0.7655\n",
      "Epoch 32/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0481 - acc: 0.9847 - val_loss: 5.4174 - val_acc: 0.7672\n",
      "Epoch 33/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0413 - acc: 0.9866 - val_loss: 5.6253 - val_acc: 0.7666\n",
      "Epoch 34/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0394 - acc: 0.9869 - val_loss: 5.7887 - val_acc: 0.7661\n",
      "Epoch 35/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0427 - acc: 0.9873 - val_loss: 6.1229 - val_acc: 0.7688\n",
      "Epoch 36/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0454 - acc: 0.9861 - val_loss: 6.6248 - val_acc: 0.7661\n",
      "Epoch 37/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0419 - acc: 0.9865 - val_loss: 6.6507 - val_acc: 0.7612- loss: 0.0414 - acc: 0.986 - ETA: 0s - loss: 0.0419 - acc: 0.986\n",
      "Epoch 38/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0338 - acc: 0.9891 - val_loss: 6.9432 - val_acc: 0.7634\n",
      "Epoch 39/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0400 - acc: 0.9870 - val_loss: 7.1326 - val_acc: 0.7639\n",
      "Epoch 40/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0391 - acc: 0.9880 - val_loss: 7.7304 - val_acc: 0.7655\n",
      "Epoch 41/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0388 - acc: 0.9869 - val_loss: 7.9123 - val_acc: 0.7650\n",
      "Epoch 42/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0328 - acc: 0.9899 - val_loss: 7.8361 - val_acc: 0.7661\n",
      "Epoch 43/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0374 - acc: 0.9869 - val_loss: 8.5991 - val_acc: 0.7747\n",
      "Epoch 44/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0376 - acc: 0.9872 - val_loss: 8.4066 - val_acc: 0.7693\n",
      "Epoch 45/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0335 - acc: 0.9889 - val_loss: 8.7954 - val_acc: 0.7731\n",
      "Epoch 46/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0411 - acc: 0.9859 - val_loss: 8.8954 - val_acc: 0.7704\n",
      "Epoch 47/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0343 - acc: 0.9889 - val_loss: 9.6614 - val_acc: 0.7817\n",
      "Epoch 48/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0401 - acc: 0.9866 - val_loss: 9.1870 - val_acc: 0.7699\n",
      "Epoch 49/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0369 - acc: 0.9876 - val_loss: 9.8553 - val_acc: 0.7785\n",
      "Epoch 50/50\n",
      "232/232 [==============================] - 1s 3ms/step - loss: 0.0360 - acc: 0.9878 - val_loss: 9.7663 - val_acc: 0.7715\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Test data\n",
      "52/52 [==============================] - 0s 939us/step - loss: 9.2271 - acc: 0.7998\n"
     ]
    }
   ],
   "source": [
    "history = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "75dbb6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 2ms/step - loss: 9.2271 - acc: 0.7998\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "fbf8db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 79.98%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], results[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c390c9fb",
   "metadata": {},
   "source": [
    "# IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3caecc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "448b9fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>hurricane_maria</td>\n",
       "      <td>912341945430798336</td>\n",
       "      <td>912341945430798336_0</td>\n",
       "      <td>11am #Maria update: tropical storm force winds...</td>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912341945...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909936612196454403</td>\n",
       "      <td>909936612196454403_0</td>\n",
       "      <td>RT @WhereTraveler: Hurricane Irma Survival Gui...</td>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099366121...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>hurricane_harvey</td>\n",
       "      <td>905581858485882881</td>\n",
       "      <td>905581858485882881_0</td>\n",
       "      <td>Thinking of doing a #DNA test? Buy a @FamilyTr...</td>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905581858...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909879746598723590</td>\n",
       "      <td>909879746598723590_0</td>\n",
       "      <td>Now blaming Hurricane Irma on Ursula the Witch...</td>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098797465...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>mexico_earthquake</td>\n",
       "      <td>910700764808564736</td>\n",
       "      <td>910700764808564736_0</td>\n",
       "      <td>Earthquake in Mexico rocks PH Embassy, no Fili...</td>\n",
       "      <td>data_image/mexico_earthquake/21_9_2017/9107007...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 event_name            tweet_id              image_id  \\\n",
       "0      california_wildfires  917791291823591425  917791291823591425_0   \n",
       "1      california_wildfires  917791291823591425  917791291823591425_1   \n",
       "2      california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3      california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4      california_wildfires  917793137925459968  917793137925459968_2   \n",
       "...                     ...                 ...                   ...   \n",
       "13603       hurricane_maria  912341945430798336  912341945430798336_0   \n",
       "13604        hurricane_irma  909936612196454403  909936612196454403_0   \n",
       "13605      hurricane_harvey  905581858485882881  905581858485882881_0   \n",
       "13606        hurricane_irma  909879746598723590  909879746598723590_0   \n",
       "13607     mexico_earthquake  910700764808564736  910700764808564736_0   \n",
       "\n",
       "                                              tweet_text  \\\n",
       "0      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "1      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "2      RT @KAKEnews: California wildfires destroy mor...   \n",
       "3      RT @KAKEnews: California wildfires destroy mor...   \n",
       "4      RT @KAKEnews: California wildfires destroy mor...   \n",
       "...                                                  ...   \n",
       "13603  11am #Maria update: tropical storm force winds...   \n",
       "13604  RT @WhereTraveler: Hurricane Irma Survival Gui...   \n",
       "13605  Thinking of doing a #DNA test? Buy a @FamilyTr...   \n",
       "13606  Now blaming Hurricane Irma on Ursula the Witch...   \n",
       "13607  Earthquake in Mexico rocks PH Embassy, no Fili...   \n",
       "\n",
       "                                                   image            label  \\\n",
       "0      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "1      data_image/california_wildfires/10_10_2017/917...  not_informative   \n",
       "2      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "3      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "4      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "...                                                  ...              ...   \n",
       "13603  data_image/hurricane_maria/25_9_2017/912341945...      informative   \n",
       "13604  data_image/hurricane_irma/19_9_2017/9099366121...      informative   \n",
       "13605  data_image/hurricane_harvey/7_9_2017/905581858...  not_informative   \n",
       "13606  data_image/hurricane_irma/18_9_2017/9098797465...      informative   \n",
       "13607  data_image/mexico_earthquake/21_9_2017/9107007...      informative   \n",
       "\n",
       "        label_text      label_image label_text_image  \n",
       "0      informative      informative         Positive  \n",
       "1      informative  not_informative         Negative  \n",
       "2      informative      informative         Positive  \n",
       "3      informative      informative         Positive  \n",
       "4      informative      informative         Positive  \n",
       "...            ...              ...              ...  \n",
       "13603  informative      informative         Positive  \n",
       "13604  informative  not_informative         Negative  \n",
       "13605  informative  not_informative         Negative  \n",
       "13606  informative  not_informative         Negative  \n",
       "13607  informative      informative         Positive  \n",
       "\n",
       "[13608 rows x 9 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Downloads\\\\CRISIS_1\\\\task_informative_text_img_train.csv\", sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "99b9f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns=df[['image','label_text']]\n",
    "df2=selected_columns.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8773e7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912341945...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099366121...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905581858...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098797465...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>data_image/mexico_earthquake/21_9_2017/9107007...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image   label_text\n",
       "0      data_image/california_wildfires/10_10_2017/917...  informative\n",
       "1      data_image/california_wildfires/10_10_2017/917...  informative\n",
       "2      data_image/california_wildfires/10_10_2017/917...  informative\n",
       "3      data_image/california_wildfires/10_10_2017/917...  informative\n",
       "4      data_image/california_wildfires/10_10_2017/917...  informative\n",
       "...                                                  ...          ...\n",
       "13603  data_image/hurricane_maria/25_9_2017/912341945...  informative\n",
       "13604  data_image/hurricane_irma/19_9_2017/9099366121...  informative\n",
       "13605  data_image/hurricane_harvey/7_9_2017/905581858...  informative\n",
       "13606  data_image/hurricane_irma/18_9_2017/9098797465...  informative\n",
       "13607  data_image/mexico_earthquake/21_9_2017/9107007...  informative\n",
       "\n",
       "[13608 rows x 2 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4e5bf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = []\n",
    "\n",
    "for i in range(df2.shape[0]):\n",
    "    if (df2.iloc[i]['label_text']=='informative'):\n",
    "        code.append(1)\n",
    "    elif (df2.iloc[i]['label_text']=='not_informative'):\n",
    "        code.append(0)\n",
    "    else:\n",
    "        poke_type.append('NaN')\n",
    "        code.append('NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f1f13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['code'] = code\n",
    "new_df1 = df2\n",
    "\n",
    "new_df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f885e1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label_image</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912341945...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099366121...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905581858...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098797465...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>data_image/mexico_earthquake/21_9_2017/9107007...</td>\n",
       "      <td>informative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image      label_image  \\\n",
       "0      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "1      data_image/california_wildfires/10_10_2017/917...  not_informative   \n",
       "2      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "3      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "4      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "...                                                  ...              ...   \n",
       "13603  data_image/hurricane_maria/25_9_2017/912341945...      informative   \n",
       "13604  data_image/hurricane_irma/19_9_2017/9099366121...  not_informative   \n",
       "13605  data_image/hurricane_harvey/7_9_2017/905581858...  not_informative   \n",
       "13606  data_image/hurricane_irma/18_9_2017/9098797465...  not_informative   \n",
       "13607  data_image/mexico_earthquake/21_9_2017/9107007...      informative   \n",
       "\n",
       "       code  \n",
       "0         1  \n",
       "1         0  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "13603     1  \n",
       "13604     0  \n",
       "13605     0  \n",
       "13606     0  \n",
       "13607     1  \n",
       "\n",
       "[13608 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59236df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df1.drop('label_image',axis = 1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "05bd55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df1, test_df1= train_test_split(df2, test_size=0.2, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7542a83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6334</th>\n",
       "      <td>data_image/hurricane_maria/24_9_2017/911922812...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>data_image/hurricane_maria/1_10_2017/914599836...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>data_image/hurricane_maria/23_10_2017/92227041...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>data_image/srilanka_floods/28_6_2017/880086747...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098461211...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912389558...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>data_image/hurricane_maria/23_9_2017/911712278...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6420</th>\n",
       "      <td>data_image/hurricane_irma/17_9_2017/9093950452...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>data_image/srilanka_floods/29_6_2017/880461341...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7692</th>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9097503863...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image       label_text\n",
       "6334   data_image/hurricane_maria/24_9_2017/911922812...  not_informative\n",
       "10738  data_image/hurricane_maria/1_10_2017/914599836...      informative\n",
       "3986   data_image/hurricane_maria/23_10_2017/92227041...  not_informative\n",
       "438    data_image/srilanka_floods/28_6_2017/880086747...  not_informative\n",
       "2803   data_image/hurricane_irma/18_9_2017/9098461211...      informative\n",
       "...                                                  ...              ...\n",
       "1775   data_image/hurricane_maria/25_9_2017/912389558...      informative\n",
       "1645   data_image/hurricane_maria/23_9_2017/911712278...      informative\n",
       "6420   data_image/hurricane_irma/17_9_2017/9093950452...      informative\n",
       "13236  data_image/srilanka_floods/29_6_2017/880461341...  not_informative\n",
       "7692   data_image/hurricane_irma/18_9_2017/9097503863...      informative\n",
       "\n",
       "[2722 rows x 2 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ed246e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11191</th>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905935224...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7707</th>\n",
       "      <td>data_image/mexico_earthquake/27_9_2017/9130488...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>data_image/srilanka_floods/1_6_2017/8701060807...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1724</th>\n",
       "      <td>data_image/hurricane_maria/24_9_2017/912102963...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>data_image/hurricane_maria/30_9_2017/914272099...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099325257...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9097972497...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6618</th>\n",
       "      <td>data_image/hurricane_harvey/4_9_2017/904850093...</td>\n",
       "      <td>not_informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>data_image/hurricane_irma/7_9_2017/90571203508...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13444</th>\n",
       "      <td>data_image/iraq_iran_earthquake/14_11_2017/930...</td>\n",
       "      <td>informative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10886 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image       label_text\n",
       "11191  data_image/hurricane_harvey/7_9_2017/905935224...  not_informative\n",
       "7707   data_image/mexico_earthquake/27_9_2017/9130488...      informative\n",
       "182    data_image/srilanka_floods/1_6_2017/8701060807...  not_informative\n",
       "1724   data_image/hurricane_maria/24_9_2017/912102963...      informative\n",
       "7911   data_image/hurricane_maria/30_9_2017/914272099...      informative\n",
       "...                                                  ...              ...\n",
       "2934   data_image/hurricane_irma/19_9_2017/9099325257...      informative\n",
       "10383  data_image/hurricane_irma/18_9_2017/9097972497...      informative\n",
       "6618   data_image/hurricane_harvey/4_9_2017/904850093...  not_informative\n",
       "8510   data_image/hurricane_irma/7_9_2017/90571203508...      informative\n",
       "13444  data_image/iraq_iran_earthquake/14_11_2017/930...      informative\n",
       "\n",
       "[10886 rows x 2 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bed019b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "429ba16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=150\n",
    "ncolumns=150\n",
    "channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "628f342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(path,lab):\n",
    "    X5=[]\n",
    "    y5=[]\n",
    "    for image in path:\n",
    "        X5.append(cv2.resize(cv2.imread(image,cv2.IMREAD_COLOR),(nrows,ncolumns),interpolation=cv2.INTER_CUBIC))\n",
    "    for la in lab:\n",
    "        if(la=='informative'):\n",
    "            y5.append(1)\n",
    "        else:\n",
    "            y5.append(0)\n",
    "    return X5,y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e0cd81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=read(train_df1['image'],train_df1['label_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28815d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[216, 203, 159],\n",
       "        [180, 162, 132],\n",
       "        [182, 179, 147],\n",
       "        ...,\n",
       "        [ 39,  49,  47],\n",
       "        [117, 119, 129],\n",
       "        [124, 124, 140]],\n",
       "\n",
       "       [[218, 195, 169],\n",
       "        [176, 156, 125],\n",
       "        [119,  98,  69],\n",
       "        ...,\n",
       "        [102, 109, 120],\n",
       "        [122, 124, 142],\n",
       "        [111, 116, 126]],\n",
       "\n",
       "       [[223, 197, 175],\n",
       "        [214, 191, 161],\n",
       "        [201, 176, 145],\n",
       "        ...,\n",
       "        [106, 113, 131],\n",
       "        [119, 123, 141],\n",
       "        [116, 122, 135]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 97,  91,  78],\n",
       "        [107,  98,  85],\n",
       "        [104,  95,  82],\n",
       "        ...,\n",
       "        [173, 158, 162],\n",
       "        [172, 178, 190],\n",
       "        [141, 147, 160]],\n",
       "\n",
       "       [[ 98,  91,  76],\n",
       "        [109,  98,  84],\n",
       "        [110,  99,  85],\n",
       "        ...,\n",
       "        [183, 162, 155],\n",
       "        [174, 172, 175],\n",
       "        [150, 148, 151]],\n",
       "\n",
       "       [[107,  96,  82],\n",
       "        [109,  98,  84],\n",
       "        [112,  99,  83],\n",
       "        ...,\n",
       "        [188, 154, 144],\n",
       "        [200, 189, 183],\n",
       "        [151, 143, 146]]], dtype=uint8)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b596c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4dc65c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "23cd21c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10886, 150, 150, 3)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "af6a4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "97d4c796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10886,)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cbf1bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_val1, y_train1, y_val1 = train_test_split(X, y, random_state=42, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "38375bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9253, 150, 150, 3)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "5f24ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    \n",
    "    rescale=1./255., \n",
    "    rotation_range=40, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=.2, \n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True, \n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "167af045",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow(\n",
    "    X_train1,y_train1,batch_size=32, \n",
    "   \n",
    ")\n",
    "validation_generator= train_datagen.flow(\n",
    "    X_val1,y_val1,batch_size=32, \n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "efd396cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.Sequential()\n",
    "\n",
    "# Convolutional layer and maxpool layer 1\n",
    "model1.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model1.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 2\n",
    "model1.add(keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 3\n",
    "model1.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model1.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 4\n",
    "model1.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model1.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# This layer flattens the resulting image array to 1D array\n",
    "model1.add(keras.layers.Flatten())\n",
    "# model1.add(keras.dropout(0.5))\n",
    "# Hidden layer with 512 neurons and Rectified Linear Unit activation function \n",
    "model1.add(keras.layers.Dense(512,activation='relu'))\n",
    "\n",
    "# Output layer with single neuron which gives 0 for Cat or 1 for Dog \n",
    "#Here we use sigmoid activation function which makes our model output to lie between 0 and 1\n",
    "model1.add(keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ebf5e2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 74, 74, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 34, 34, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 17, 17, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 15, 15, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 512)               3211776   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3a05925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "59e16b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-189-c36b89fbdcc5>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history1=model1.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "290/290 [==============================] - 208s 718ms/step - loss: 0.6030 - accuracy: 0.7073 - val_loss: 0.5925 - val_accuracy: 0.7152\n",
      "Epoch 2/2\n",
      "290/290 [==============================] - 206s 710ms/step - loss: 0.6017 - accuracy: 0.7073 - val_loss: 0.5883 - val_accuracy: 0.7152\n"
     ]
    }
   ],
   "source": [
    "history1=model1.fit_generator(train_generator,\n",
    "         \n",
    "         epochs = 2,\n",
    "         validation_data = validation_generator\n",
    "       \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "8520f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-190-2d509f92f66e>:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  results = model1.evaluate_generator(validation_generator)\n"
     ]
    }
   ],
   "source": [
    "results = model1.evaluate_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "16abf534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  71.52479887008667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", results[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4c4983cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 10s 548ms/step - loss: 2.2644 - accuracy: 0.7012\n"
     ]
    }
   ],
   "source": [
    "results1 = model1.evaluate(X_val1, y_val1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "26033c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  70.11635303497314\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", results1[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c812e8",
   "metadata": {},
   "source": [
    "# COMBINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "77fe82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers import Conv2D, Conv1D, MaxPooling2D, Reshape, Concatenate, Dropout , MaxPooling1D, Flatten\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d27bde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = model.output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "53bf2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2=model1.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "582a0fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate \n",
    "from keras.layers import Concatenate \n",
    "\n",
    "x = concatenate([out1, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6e82a070",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = Dense(1, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6f16e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(inputs=[model.input, model1.input], outputs=[out])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "03580ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " conv2d_4_input (InputLayer)    [(None, 150, 150, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 148, 148, 32  896         ['conv2d_4_input[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 74, 74, 32)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 72, 72, 64)   18496       ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " embedding_2_input (InputLayer)  [(None, 10)]        0           []                               \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 36, 36, 64)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 10, 32)       320000      ['embedding_2_input[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 34, 34, 128)  73856       ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 320)          0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 17, 17, 128)  0          ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 16)           5136        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 15, 15, 128)  147584      ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 16)           0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 7, 7, 128)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 8)            136         ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 6272)         0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 8)            0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 512)          3211776     ['flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            9           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 1)            513         ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 2)            0           ['dense_11[0][0]',               \n",
      "                                                                  'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            3           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,778,405\n",
      "Trainable params: 3,778,405\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "79ba2283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 193s 659ms/step - loss: 0.5904 - accuracy: 0.7073 - val_loss: 0.5779 - val_accuracy: 0.7152\n"
     ]
    }
   ],
   "source": [
    "history2=model2.fit([X_train,X_train1], y_train,epochs=1,validation_data=([X_test,X_val1], y_val1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2c7968da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 7s 535ms/step - loss: 0.5779 - accuracy: 0.7152\n"
     ]
    }
   ],
   "source": [
    "results1 = model2.evaluate([X_test,X_val1], y_val1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f4f4c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  71.52479887008667\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \", results1[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1dadd398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_name</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>label_image</th>\n",
       "      <th>label_text_image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_0</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917791291823591425</td>\n",
       "      <td>917791291823591425_1</td>\n",
       "      <td>RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_0</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_1</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>california_wildfires</td>\n",
       "      <td>917793137925459968</td>\n",
       "      <td>917793137925459968_2</td>\n",
       "      <td>RT @KAKEnews: California wildfires destroy mor...</td>\n",
       "      <td>data_image/california_wildfires/10_10_2017/917...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13603</th>\n",
       "      <td>hurricane_maria</td>\n",
       "      <td>912341945430798336</td>\n",
       "      <td>912341945430798336_0</td>\n",
       "      <td>11am #Maria update: tropical storm force winds...</td>\n",
       "      <td>data_image/hurricane_maria/25_9_2017/912341945...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13604</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909936612196454403</td>\n",
       "      <td>909936612196454403_0</td>\n",
       "      <td>RT @WhereTraveler: Hurricane Irma Survival Gui...</td>\n",
       "      <td>data_image/hurricane_irma/19_9_2017/9099366121...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13605</th>\n",
       "      <td>hurricane_harvey</td>\n",
       "      <td>905581858485882881</td>\n",
       "      <td>905581858485882881_0</td>\n",
       "      <td>Thinking of doing a #DNA test? Buy a @FamilyTr...</td>\n",
       "      <td>data_image/hurricane_harvey/7_9_2017/905581858...</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13606</th>\n",
       "      <td>hurricane_irma</td>\n",
       "      <td>909879746598723590</td>\n",
       "      <td>909879746598723590_0</td>\n",
       "      <td>Now blaming Hurricane Irma on Ursula the Witch...</td>\n",
       "      <td>data_image/hurricane_irma/18_9_2017/9098797465...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13607</th>\n",
       "      <td>mexico_earthquake</td>\n",
       "      <td>910700764808564736</td>\n",
       "      <td>910700764808564736_0</td>\n",
       "      <td>Earthquake in Mexico rocks PH Embassy, no Fili...</td>\n",
       "      <td>data_image/mexico_earthquake/21_9_2017/9107007...</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>informative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13608 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 event_name            tweet_id              image_id  \\\n",
       "0      california_wildfires  917791291823591425  917791291823591425_0   \n",
       "1      california_wildfires  917791291823591425  917791291823591425_1   \n",
       "2      california_wildfires  917793137925459968  917793137925459968_0   \n",
       "3      california_wildfires  917793137925459968  917793137925459968_1   \n",
       "4      california_wildfires  917793137925459968  917793137925459968_2   \n",
       "...                     ...                 ...                   ...   \n",
       "13603       hurricane_maria  912341945430798336  912341945430798336_0   \n",
       "13604        hurricane_irma  909936612196454403  909936612196454403_0   \n",
       "13605      hurricane_harvey  905581858485882881  905581858485882881_0   \n",
       "13606        hurricane_irma  909879746598723590  909879746598723590_0   \n",
       "13607     mexico_earthquake  910700764808564736  910700764808564736_0   \n",
       "\n",
       "                                              tweet_text  \\\n",
       "0      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "1      RT @Cal_OES: PLS SHARE: Weâ€™re capturing wild...   \n",
       "2      RT @KAKEnews: California wildfires destroy mor...   \n",
       "3      RT @KAKEnews: California wildfires destroy mor...   \n",
       "4      RT @KAKEnews: California wildfires destroy mor...   \n",
       "...                                                  ...   \n",
       "13603  11am #Maria update: tropical storm force winds...   \n",
       "13604  RT @WhereTraveler: Hurricane Irma Survival Gui...   \n",
       "13605  Thinking of doing a #DNA test? Buy a @FamilyTr...   \n",
       "13606  Now blaming Hurricane Irma on Ursula the Witch...   \n",
       "13607  Earthquake in Mexico rocks PH Embassy, no Fili...   \n",
       "\n",
       "                                                   image            label  \\\n",
       "0      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "1      data_image/california_wildfires/10_10_2017/917...  not_informative   \n",
       "2      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "3      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "4      data_image/california_wildfires/10_10_2017/917...      informative   \n",
       "...                                                  ...              ...   \n",
       "13603  data_image/hurricane_maria/25_9_2017/912341945...      informative   \n",
       "13604  data_image/hurricane_irma/19_9_2017/9099366121...      informative   \n",
       "13605  data_image/hurricane_harvey/7_9_2017/905581858...  not_informative   \n",
       "13606  data_image/hurricane_irma/18_9_2017/9098797465...      informative   \n",
       "13607  data_image/mexico_earthquake/21_9_2017/9107007...      informative   \n",
       "\n",
       "        label_text      label_image label_text_image  \n",
       "0      informative      informative         Positive  \n",
       "1      informative  not_informative         Negative  \n",
       "2      informative      informative         Positive  \n",
       "3      informative      informative         Positive  \n",
       "4      informative      informative         Positive  \n",
       "...            ...              ...              ...  \n",
       "13603  informative      informative         Positive  \n",
       "13604  informative  not_informative         Negative  \n",
       "13605  informative  not_informative         Negative  \n",
       "13606  informative  not_informative         Negative  \n",
       "13607  informative      informative         Positive  \n",
       "\n",
       "[13608 rows x 9 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df8 = pd.read_csv(\"C:\\\\Users\\\\HP\\\\Downloads\\\\CRISIS_1\\\\task_informative_text_img_train.csv\", sep=',')\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "77b4e5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "informative        9638\n",
       "not_informative    3970\n",
       "Name: label_text, dtype: int64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8['label_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "78b28d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X8=df8['tweet_text']\n",
    "y8=df8['label_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8ac3f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training target statistics: Counter({'informative': 6793, 'not_informative': 2732})\n",
      "Testing target statistics: Counter({'informative': 2845, 'not_informative': 1238})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "X_train8, X_test8, y_train8, y_test8 = train_test_split(X8, y8, test_size=0.3, random_state=100)\n",
    "print(f\"Training target statistics: {Counter(y_train8)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "305f1372",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DistanceMetric' from 'sklearn.metrics' (C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-220-b817fac69b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# from sklearn.neighbors import DistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# over_sampler = RandomOverSampler(random_state=42)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# X_res8, y_res8 = over_sampler.fit_resample(X_train8, y_train8)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# print(f\"Training target statistics: {Counter(y_res8)}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# process, as it may not be compiled yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\combine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_docstring\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSubstitution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_neighbors_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_sampling_strategy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\utils\\_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ball_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distance_metric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsTransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRadiusNeighborsTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_distance_metric.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_DistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DistanceMetric' from 'sklearn.metrics' (C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "over_sampler = RandomOverSampler(random_state=42)\n",
    "X_res8, y_res8 = over_sampler.fit_resample(X_train8, y_train8)\n",
    "print(f\"Training target statistics: {Counter(y_res8)}\")\n",
    "print(f\"Testing target statistics: {Counter(y_test8)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "906bcedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.0)\n",
      "Collecting scikit-learn>=1.0.1\n",
      "  Downloading scikit_learn-1.0.2-cp38-cp38-win_amd64.whl (7.2 MB)\n",
      "Installing collected packages: scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.24.1\n",
      "    Uninstalling scikit-learn-0.24.1:\n",
      "      Successfully uninstalled scikit-learn-0.24.1\n",
      "Successfully installed imbalanced-learn-0.9.0 imblearn-0.0 scikit-learn-1.0.2\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "085dc075",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imbalanced_learn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-223-fcc6c9485184>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimbalanced_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnderSampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOverSampler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imbalanced_learn'"
     ]
    }
   ],
   "source": [
    "from imbalanced_learn import UnderSampler, OverSampler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "5a5a3f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced_learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.9.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced_learn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced_learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced_learn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced_learn) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from imbalanced_learn) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc426844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
